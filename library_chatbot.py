import os
import sys
import streamlit as st
import nest_asyncio

# Streamlitì—ì„œ ë¹„ë™ê¸° ì‘ì—…ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì •
nest_asyncio.apply()

# ================================
# 1. âŒ Windowsì—ì„œëŠ” pysqlite3 íŒ¨ì¹˜ ì œê±°!
# ================================
# __import__('pysqlite3')
# sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

# ================================
# 2. LangChain & Embedding & Chroma
# ================================
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory

from langchain_chroma import Chroma


# ================================
# 3. Gemini API í‚¤ ì„¤ì •
# ================================
try:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except Exception:
    st.error("âš ï¸ GOOGLE_API_KEYë¥¼ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    st.stop()


# ================================
# 4. PDF ê²½ë¡œ ë° Chroma í´ë”ëª…
# ================================
PDF_PATH = r"/mount/src/librarychatbot_gemini/ì¸ì²œ ì„¬ ê°¯ë²Œì— ëŒ€í•œ ìƒíƒœì  ê°€ì¹˜í™” ë°©ì•ˆê³¼ ì ìš©.pdf"
PDF_NAME = os.path.splitext(os.path.basename(PDF_PATH))[0]
VECTOR_DIR = f"./chroma_db_{PDF_NAME}"


# ================================
# 5. Streamlit ìºì‹œ ì´ˆê¸°í™” ë²„íŠ¼
# ================================
if st.button("ğŸ”„ ìºì‹œ ë° ì„ë² ë”© ë°ì´í„° ì´ˆê¸°í™”"):
    import shutil
    if os.path.exists(VECTOR_DIR):
        shutil.rmtree(VECTOR_DIR)
        st.success("ğŸ—‘ï¸ ê¸°ì¡´ ChromaDB ë°ì´í„° ì‚­ì œ ì™„ë£Œ!")
    st.cache_resource.clear()
    st.success("â™»ï¸ Streamlit ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ! ì•±ì„ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")


# ================================
# 6. PDF load & split
# ================================
@st.cache_resource
def load_and_split_pdf(file_path):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()


# ================================
# 7. ChromaDB ìƒì„± í•¨ìˆ˜
# ================================
client_settings = {
    "chroma_tenant": "default_tenant",
    "chroma_collection": "default"
}

@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    split_docs = text_splitter.split_documents(_docs)
    st.info(f"ğŸ“„ {len(split_docs)}ê°œ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    st.info("ğŸ”¢ ë²¡í„° ì„ë² ë”© ìƒì„± ì¤‘...")

    vectorstore = Chroma.from_documents(
        split_docs,
        embedding=embeddings,
        persist_directory=VECTOR_DIR,
        collection_name="default"
    )

    st.success("ğŸ’¾ ìƒˆë¡œìš´ ChromaDB ì €ì¥ ì™„ë£Œ!")
    return vectorstore


# ================================
# 8. ê¸°ì¡´ DB ë¡œë“œ or ìƒì„±
# ================================
@st.cache_resource
def get_vectorstore(_docs):
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    if os.path.exists(VECTOR_DIR):
        st.info("ğŸ“‚ ê¸°ì¡´ ChromaDB ë¡œë“œ ì¤‘...")
        return Chroma(
            persist_directory=VECTOR_DIR,
            embedding_function=embeddings,
            collection_name="default",
            client_settings={ 
                "chroma_tenant": "default_tenant",
                "chroma_collection": "default"
            }
        )
    else:
        return create_vector_store(_docs)


# ================================
# 9. RAG ì²´ì¸ ì´ˆê¸°í™”
# ================================
@st.cache_resource
def initialize_components(selected_model):
    pages = load_and_split_pdf(PDF_PATH)
    vectorstore = get_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    # ---- ì§ˆë¬¸ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸ ----
    contextualize_q_system_prompt = """
    Given a chat history and the latest user question which might 
    reference context in the chat history, formulate a standalone 
    question. Do NOT answer the question.
    """

    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("history"),
        ("human", "{input}")
    ])

    # ---- QA í”„ë¡¬í”„íŠ¸ ----
    qa_system_prompt = """
    You are an assistant for question-answering tasks.
    Use the retrieved context to answer the question.
    If you donâ€™t know the answer, just say so.
    Answer in Korean with natural emojis.
    {context}
    """

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        MessagesPlaceholder("history"),
        ("human", "{input}")
    ])

    llm = ChatGoogleGenerativeAI(
        model=selected_model,
        temperature=0.7,
        convert_system_message_to_human=True
    )

    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    return rag_chain


# ================================
# 10. UI
# ================================
st.header("ì¸ì²œ ì„¬ ê°¯ë²Œì— ëŒ€í•œ ìƒíƒœì  ê°€ì¹˜í™” ë°©ì•ˆê³¼ ì ìš©")

if not os.path.exists(VECTOR_DIR):
    st.info("ğŸ”„ ì²« ì‹¤í–‰ì…ë‹ˆë‹¤. PDFë¥¼ ì„ë² ë”© ì¤‘ì…ë‹ˆë‹¤...")
else:
    st.info(f"ğŸ“‚ '{PDF_NAME}' ë²¡í„°DB ë¡œë“œ ì™„ë£Œ")


option = st.selectbox(
    "Select Gemini Model",
    ("gemini-2.0-flash-exp", "gemini-2.5-flash", "gemini-2.0-flash-lite"),
    help="Gemini 2.0 Flash ì¶”ì²œ!"
)

try:
    with st.spinner("ì±—ë´‡ ì´ˆê¸°í™” ì¤‘..."):
        rag_chain = initialize_components(option)
    st.success("âœ… ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ!")
except Exception as e:
    st.error(f"âš ï¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
    st.stop()


# ================================
# 11. ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥
# ================================
chat_history = StreamlitChatMessageHistory(key="chat_messages")

conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)


# ================================
# 12. ê¸°ì¡´ ë©”ì‹œì§€ ì¶œë ¥
# ================================
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)


# ================================
# 13. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
# ================================
if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.chat_message("human").write(user_input)

    with st.chat_message("ai"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            response = conversational_rag_chain.invoke(
                {"input": user_input},
                config={"configurable": {"session_id": "safe_sea_chat"}}
            )

            answer = response["answer"]
            st.write(answer)

            with st.expander("ğŸ“˜ ì°¸ê³  ë¬¸ì„œ"):
                for doc in response["context"]:
                    st.markdown(doc.metadata.get("source", "ë¬¸ì„œ ì¶œì²˜ ì—†ìŒ"))
